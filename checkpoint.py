from torch import nn
import torch

from config import CKPT_DIR, KEEP_LAST, RUN_ID, USE_LORA
from lora import _remap_old_linear_keys_to_lora

def _model_state(m: nn.Module):
    return m.module.state_dict() if hasattr(m, "module") else m.state_dict()
def _load_model_state(m: nn.Module, sd, strict: bool = True):
    (m.module if hasattr(m, "module") else m).load_state_dict(sd, strict=strict)

def save_checkpoint(tag:str, model, opt, scaler, global_step:int, epoch:int, meta:dict):
    ckpt = {
        "tag": tag,
        "global_step": global_step,
        "epoch": epoch,
        "model": _model_state(model),
        "optimizer": opt.state_dict() if opt is not None else None,
        "scaler": (scaler.state_dict() if scaler is not None else None),
        "meta": meta,
    }
    path = CKPT_DIR / f"{RUN_ID}_{tag}_{global_step:09d}.pt"
    torch.save(ckpt, path)
    # ä¸–ä»£æ•´ç†ï¼ˆlatestç³»ã®ã¿å¯¾è±¡ï¼‰
    if tag == "latest":
        latest = sorted(CKPT_DIR.glob(f"{RUN_ID}_latest_*.pt"))
        for p in latest[:-KEEP_LAST]:
            p.unlink(missing_ok=True)
    return path
# 1) state_dict ãƒ­ãƒ¼ãƒ‰ã®å¯è¦–åŒ–
def _debug_report_load(ret):
    try:
        print(f"â–¶ load_state: missing={len(ret.missing_keys)} unexpected={len(ret.unexpected_keys)}")
        if ret.missing_keys[:5]: print("  missing(head):", ret.missing_keys[:5])
        if ret.unexpected_keys[:5]: print("  unexpected(head):", ret.unexpected_keys[:5])
    except Exception:
        pass


def _strip_module_prefix(sd: dict):
    # DataParallel ã§ä¿å­˜ã•ã‚ŒãŸ ckpt ã® "module." ã‚’å‰¥ãŒã™
    if any(k.startswith("module.") for k in sd.keys()):
        return {k.replace("module.", "", 1): v for k, v in sd.items()} 
    return sd
def try_resume(model, opt, scaler):
    cands = sorted(CKPT_DIR.glob("*_latest_*.pt"))
    if not cands:
        return 0, 1, None
    path = cands[-1]
    ckpt = torch.load(path, map_location="cpu")
    sd = _strip_module_prefix(ckpt["model"])  # â˜…ã“ã“è¿½åŠ 


    # ---- LoRAã®æœ‰ç„¡ã‚’è‡ªå‹•å¸å ----
    has_lora_in_ckpt = any(".lora_A." in k or ".lora_B." in k or ".base." in k for k in sd.keys())
    if USE_LORA and not has_lora_in_ckpt:
        # æ—§(éLoRA) â†’ æ–°(LoRA) ã¸ã‚­ãƒ¼åã‚’ç§»ã—æ›¿ãˆ
        sd = _remap_old_linear_keys_to_lora(sd, model)
        _load_model_state(model, sd, strict=False)
    elif (not USE_LORA) and has_lora_in_ckpt:
        # æ–°(LoRA) ckpt ã‚’ éLoRAãƒ¢ãƒ‡ãƒ«ã«èª­ã‚€ â†’ ä½™å‰°ã‚­ãƒ¼ã‚’ç„¡è¦–
        _load_model_state(model, sd, strict=False)
    else:
        # åŒä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåŒå£«
        _load_model_state(model, sd, strict=False)
# try_resumeã® load_model_state å‘¼ã³å‡ºã—å¾Œã«:
    ret = (model.module if hasattr(model, "module") else model).load_state_dict(sd, strict=False)
    _debug_report_load(ret)
    if (opt is not None) and (ckpt.get("optimizer") is not None):
        try:
            opt.load_state_dict(ckpt["optimizer"])
        except Exception as e:
            print(f"âš  optimizer state load skipped: {e}")
    if (scaler is not None) and (ckpt.get("scaler") is not None):
        try:
            scaler.load_state_dict(ckpt["scaler"])
        except Exception as e:
            print(f"âš  scaler state load skipped: {e}")

    print(f"ğŸ” Resumed from: {path.name} | step={ckpt['global_step']} epoch={ckpt['epoch']}")
    return int(ckpt["global_step"]), int(ckpt["epoch"] + 1), ckpt["meta"].get("best_metric")
